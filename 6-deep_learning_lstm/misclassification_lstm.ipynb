{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bd4749c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# --- 1. Define Constants and Features ---\n",
    "FILE_NAME = '../4-prep_model_data/modelling_data.csv'\n",
    "TARGET_COL = 'stars_x'\n",
    "GROUP_COL = 'business_id'\n",
    "TEXT_COL = 'text'\n",
    "\n",
    "# The additional features you requested to include\n",
    "BOOLEAN_F = ['has_exclamation', 'has_question', 'is_shouting']\n",
    "CATEGORICAL_F = ['food_sentiment', 'service_sentiment', 'atmosphere_sentiment', 'overall_sentiment']\n",
    "NUMERICAL_F = ['grade_level']\n",
    "\n",
    "# All features that will be used as input (X)\n",
    "ALL_INPUT_FEATURES = [TEXT_COL] + BOOLEAN_F + CATEGORICAL_F + NUMERICAL_F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce2e795e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from '../4-prep_model_data/modelling_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Load and Prepare Data ---\n",
    "try:\n",
    "    df = pd.read_csv(FILE_NAME)\n",
    "    print(f\"Data loaded successfully from '{FILE_NAME}'.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File '{FILE_NAME}' not found. Please ensure the file is uploaded.\")\n",
    "    # Exit the script if the file cannot be loaded\n",
    "    exit()\n",
    "\n",
    "# Data Cleaning and Preparation for Robust Modeling\n",
    "df.dropna(subset=[TARGET_COL, GROUP_COL, TEXT_COL], inplace=True)\n",
    "# Fill NaNs for categorical/boolean/numerical columns to prevent data loss in the remaining rows\n",
    "df[CATEGORICAL_F] = df[CATEGORICAL_F].fillna('missing_category')\n",
    "df[BOOLEAN_F] = df[BOOLEAN_F].fillna(False)\n",
    "df[NUMERICAL_F] = df[NUMERICAL_F].fillna(df[NUMERICAL_F].mean()) # Fill numerical NaNs with the mean\n",
    "\n",
    "# Define X, y, and groups after cleaning\n",
    "y = df[TARGET_COL] \n",
    "X = df[ALL_INPUT_FEATURES]\n",
    "groups = df[GROUP_COL] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80a5d47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Size: 37828 | Testing Set Size: 8629\n",
      "Test set unique businesses: 134\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Stratified Group Split (80/20) ---\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "try:\n",
    "    # Get the indices for the 80/20 split, respecting star rating stratification and business grouping\n",
    "    train_index, test_index = next(sgkf.split(X, y, groups))\n",
    "except ValueError as e:\n",
    "    # Fallback if a group or class is too small to stratify\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    print(\"\\nWARNING: StratifiedGroupKFold failed. Falling back to standard stratified split.\")\n",
    "    train_index, test_index = train_test_split(df.index, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Apply the indices to create the training and testing sets\n",
    "X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "business_ids_test = groups.loc[test_index] \n",
    "\n",
    "print(f\"\\nTraining Set Size: {len(X_train)} | Testing Set Size: {len(X_test)}\")\n",
    "print(f\"Test set unique businesses: {business_ids_test.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b32a41ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Serena\n",
      "[nltk_data]     Wong\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4fc91c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_tokenizer(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return [t.lower() for t in tokens if t.isalnum()]  # lowercase + remove punctuation\n",
    "\n",
    "# Build vocabulary from training set\n",
    "all_tokens = [token for text in X_train['text'] for token in nltk_tokenizer(text)]\n",
    "token_counts = Counter(all_tokens)\n",
    "\n",
    "vocab_size = 10000\n",
    "most_common = token_counts.most_common(vocab_size-2)  # leave 0=<pad>, 1=<unk>\n",
    "vocab = {word: i+2 for i, (word, _) in enumerate(most_common)}\n",
    "vocab[\"<pad>\"] = 0\n",
    "vocab[\"<unk>\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a55f09c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "\n",
    "# Load GloVe\n",
    "glove_model = api.load(\"glove-wiki-gigaword-100\")  # 100-dim embeddings\n",
    "embed_dim = 100\n",
    "\n",
    "# Build embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size, embed_dim))\n",
    "\n",
    "for word, idx in vocab.items():\n",
    "    if word in glove_model:\n",
    "        embedding_matrix[idx] = glove_model[word]\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embed_dim,))\n",
    "\n",
    "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50a4d360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequence(text, vocab, maxlen=200):\n",
    "    tokens = [t for t in nltk_tokenizer(text) if t.isalnum()]\n",
    "    seq = [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
    "    # Pad or truncate\n",
    "    if len(seq) < maxlen:\n",
    "        seq += [vocab[\"<pad>\"]] * (maxlen - len(seq))\n",
    "    else:\n",
    "        seq = seq[:maxlen]\n",
    "    return seq\n",
    "\n",
    "X_train_seq = torch.tensor([text_to_sequence(t, vocab) for t in X_train['text']], dtype=torch.long)\n",
    "X_test_seq = torch.tensor([text_to_sequence(t, vocab) for t in X_test['text']], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fe8d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "bool_cols = [\"has_exclamation\", \"has_question\", \"is_shouting\"]\n",
    "sentiment_cols = [\"food_sentiment\", \"service_sentiment\", \"atmosphere_sentiment\", \"overall_sentiment\"]\n",
    "\n",
    "numeric_col = [\"grade_level\"]\n",
    "\n",
    "\n",
    "# Fit on training data\n",
    "onehot = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_cat = onehot.fit_transform(X_train[BOOLEAN_F + CATEGORICAL_F])\n",
    "X_test_cat = onehot.transform(X_test[BOOLEAN_F + CATEGORICAL_F])\n",
    "\n",
    "X_train_num = scaler.fit_transform(X_train[numeric_col])\n",
    "X_test_num = scaler.transform(X_test[numeric_col])\n",
    "\n",
    "# Combine\n",
    "X_train_meta = np.hstack([X_train_cat, X_train_num])\n",
    "X_test_meta = np.hstack([X_test_cat, X_test_num])\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_meta = torch.tensor(X_train_meta, dtype=torch.float32)\n",
    "X_test_meta = torch.tensor(X_test_meta, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e85f12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ReviewLSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, meta_dim, lstm_hidden_dim=256, fc_hidden_dim=128, num_classes=5, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.embedding.weight = nn.Parameter(embedding_matrix)\n",
    "        self.embedding.weight.requires_grad = False  # or False to freeze\n",
    "\n",
    "        self.lstm = nn.LSTM(embed_dim, lstm_hidden_dim, batch_first=True, bidirectional=True, num_layers=2, dropout=0.3)\n",
    "        self.text_dropout = nn.Dropout(0.4)\n",
    "        self.text_fc = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden_dim*2, fc_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.meta_fc = nn.Sequential(\n",
    "            nn.Linear(meta_dim, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.combined_fc = nn.Sequential(\n",
    "            nn.Linear(fc_hidden_dim + 64, fc_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(fc_hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, text, meta):\n",
    "        embedded = self.embedding(text)\n",
    "        _, (hidden, _) = self.lstm(embedded)\n",
    "        backward_last = hidden[-1]  # last forward & backward\n",
    "        forward_last = hidden[-2]\n",
    "        hidden_cat = torch.cat([forward_last, backward_last], dim=1)\n",
    "        hidden_cat = self.text_dropout(hidden_cat)\n",
    "        text_feat = self.text_fc(hidden_cat)\n",
    "\n",
    "        meta_feat = self.meta_fc(meta)\n",
    "\n",
    "        combined = torch.cat([text_feat, meta_feat], dim=1)\n",
    "        output = self.combined_fc(combined)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba9c673c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReviewLSTMModel(\n",
       "  (embedding): Embedding(10000, 100, padding_idx=0)\n",
       "  (lstm): LSTM(100, 128, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (text_dropout): Dropout(p=0.4, inplace=False)\n",
       "  (text_fc): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (meta_fc): Sequential(\n",
       "    (0): Linear(in_features=18, out_features=64, bias=True)\n",
       "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (combined_fc): Sequential(\n",
       "    (0): Linear(in_features=320, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "embed_dim = 100\n",
    "hidden_dim = 128\n",
    "meta_dim = X_train_meta.shape[1]\n",
    "output_dim = 5\n",
    "num_epochs = 50\n",
    "batch_size = 32       # smaller batch\n",
    "accum_steps = 2       # gradient accumulation\n",
    "patience = 5          # early stopping patience\n",
    "min_delta = 1e-4  \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ReviewLSTMModel(vocab_size=vocab_size, \n",
    "                        embed_dim=embed_dim, \n",
    "                        meta_dim=meta_dim, \n",
    "                        lstm_hidden_dim=hidden_dim, \n",
    "                        fc_hidden_dim=256,\n",
    "                        num_classes=output_dim)\n",
    "\n",
    "# Load weights onto CPU\n",
    "model.load_state_dict(torch.load(\"best_model.pth\", map_location=torch.device('cpu')))\n",
    "model.eval()  # Important: set to evaluation mode\n",
    "\n",
    "# If you have meta/text tensors on CPU, you can do predictions now\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c1376eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, text_tensors, meta_features, labels):\n",
    "        self.text_tensors = text_tensors\n",
    "        self.meta_features = meta_features\n",
    "\n",
    "        if isinstance(labels, (pd.Series, pd.DataFrame)):\n",
    "            self.labels = torch.tensor(labels.values - 1, dtype=torch.long)\n",
    "        else:\n",
    "            self.labels = labels.long()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text_tensors[idx]\n",
    "        meta = self.meta_features[idx]\n",
    "        label = self.labels[idx]\n",
    "        return text, meta, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af0a6113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Shift labels 1-5 â†’ 0-4\n",
    "y_test_tensor = torch.tensor(y_test.values - 1, dtype=torch.long)\n",
    "\n",
    "# Create datasets\n",
    "test_dataset = ReviewDataset(X_test_seq, X_test_meta, y_test_tensor)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8086c080",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for text_batch, meta_batch, _ in test_loader:\n",
    "        text_batch = text_batch.to(device)\n",
    "        meta_batch = meta_batch.to(device)\n",
    "        outputs = model(text_batch, meta_batch)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "# Keep both on 0-4 scale\n",
    "y_pred = np.array(y_pred)\n",
    "y_true = y_test_tensor.numpy()  # already 0-4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81ff06f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 3, 3, 4, 3, 4, 3, 3, 1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "09620498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 4, 2, 4, 3, 3, 4, 4, 2])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fbb400e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Misclassification Analysis Complete.\n",
      "Results saved to 'misclassification_analysis_lstm.csv' for your next step.\n",
      "\n",
      "--- Model Performance Summary ---\n",
      "Accuracy: 0.6307799281492641\n",
      "Sample of Misclassified Reviews (True vs. Predicted):\n",
      "              business_id  True_Star_Rating  Predicted_Star_Rating\n",
      "0  V7IHpr1xzFIf_jp876HoAw                 4                      2\n",
      "1  V7IHpr1xzFIf_jp876HoAw                 5                      3\n",
      "2  V7IHpr1xzFIf_jp876HoAw                 5                      3\n",
      "4  s9G06FPW74Prlp8s1h5nEA                 5                      4\n",
      "5  s9G06FPW74Prlp8s1h5nEA                 4                      3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# Create the final DataFrame for misclassification analysis\n",
    "misclassification_df = pd.DataFrame({\n",
    "    'business_id': business_ids_test.values,\n",
    "    'Review_Text': X_test[TEXT_COL].values,\n",
    "    'True_Star_Rating': y_test.values,\n",
    "    'Predicted_Star_Rating': y_pred\n",
    "})\n",
    "\n",
    "misclassification_df['Is_Misclassified'] = (misclassification_df['True_Star_Rating'] != misclassification_df['Predicted_Star_Rating'])\n",
    "\n",
    "# Save the DataFrame to a file for your detailed analysis\n",
    "OUTPUT_FILE = 'misclassification_analysis_lstm.csv'\n",
    "misclassification_df.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(\"\\nMisclassification Analysis Complete.\")\n",
    "print(f\"Results saved to '{OUTPUT_FILE}' for your next step.\")\n",
    "print(\"\\n--- Model Performance Summary ---\")\n",
    "print(\"Accuracy:\", classification_report(y_true, y_pred, output_dict=True)['accuracy'])\n",
    "print(\"Sample of Misclassified Reviews (True vs. Predicted):\")\n",
    "print(misclassification_df[misclassification_df['Is_Misclassified']][['business_id', 'True_Star_Rating', 'Predicted_Star_Rating']].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
