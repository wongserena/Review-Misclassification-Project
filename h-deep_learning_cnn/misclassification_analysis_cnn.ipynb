{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711c352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# --- 1. Define Constants and Features ---\n",
    "FILE_NAME = '../4-prep_model_data/modelling_data.csv'\n",
    "TARGET_COL = 'stars_x'\n",
    "GROUP_COL = 'business_id'\n",
    "TEXT_COL = 'text'\n",
    "\n",
    "BOOLEAN_F = ['has_exclamation', 'has_question', 'is_shouting']\n",
    "CATEGORICAL_F = ['food_sentiment', 'service_sentiment', 'atmosphere_sentiment', 'overall_sentiment']\n",
    "NUMERICAL_F = ['grade_level']\n",
    "\n",
    "# All features that will be used as input (X)\n",
    "ALL_INPUT_FEATURES = [TEXT_COL] + BOOLEAN_F + CATEGORICAL_F + NUMERICAL_F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0cca746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from '../4-prep_model_data/modelling_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Load and Prepare Data ---\n",
    "try:\n",
    "    df = pd.read_csv(FILE_NAME)\n",
    "    print(f\"Data loaded successfully from '{FILE_NAME}'.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File '{FILE_NAME}' not found. Please ensure the file is uploaded.\")\n",
    "    # Exit the script if the file cannot be loaded\n",
    "    exit()\n",
    "\n",
    "# Data Cleaning and Preparation for Robust Modeling\n",
    "df.dropna(subset=[TARGET_COL, GROUP_COL, TEXT_COL], inplace=True)\n",
    "# Fill NaNs for categorical/boolean/numerical columns to prevent data loss in the remaining rows\n",
    "df[CATEGORICAL_F] = df[CATEGORICAL_F].fillna('missing_category')\n",
    "df[BOOLEAN_F] = df[BOOLEAN_F].fillna(False)\n",
    "df[NUMERICAL_F] = df[NUMERICAL_F].fillna(df[NUMERICAL_F].mean()) # Fill numerical NaNs with the mean\n",
    "\n",
    "# Define X, y, and groups after cleaning\n",
    "y = df[TARGET_COL] \n",
    "X = df[ALL_INPUT_FEATURES]\n",
    "groups = df[GROUP_COL] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e77bb1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Size: 37828 | Testing Set Size: 8629\n",
      "Test set unique businesses: 134\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Stratified Group Split (80/20) ---\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "try:\n",
    "    # Get the indices for the 80/20 split, respecting star rating stratification and business grouping\n",
    "    train_index, test_index = next(sgkf.split(X, y, groups))\n",
    "except ValueError as e:\n",
    "    # Fallback if a group or class is too small to stratify\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    print(\"\\nWARNING: StratifiedGroupKFold failed. Falling back to standard stratified split.\")\n",
    "    train_index, test_index = train_test_split(df.index, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Apply the indices to create the training and testing sets\n",
    "X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "business_ids_test = groups.loc[test_index] \n",
    "\n",
    "print(f\"\\nTraining Set Size: {len(X_train)} | Testing Set Size: {len(X_test)}\")\n",
    "print(f\"Test set unique businesses: {business_ids_test.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7668cfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Serena\n",
      "[nltk_data]     Wong\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e06db2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_tokenizer(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return [t.lower() for t in tokens if t.isalnum()]  # lowercase + remove punctuation\n",
    "\n",
    "# Build vocabulary from training set\n",
    "all_tokens = [token for text in X_train['text'] for token in nltk_tokenizer(text)]\n",
    "token_counts = Counter(all_tokens)\n",
    "\n",
    "vocab_size = 10000\n",
    "most_common = token_counts.most_common(vocab_size-2)  # leave 0=<pad>, 1=<unk>\n",
    "vocab = {word: i+2 for i, (word, _) in enumerate(most_common)}\n",
    "vocab[\"<pad>\"] = 0\n",
    "vocab[\"<unk>\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8990ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "\n",
    "# Load GloVe\n",
    "glove_model = api.load(\"glove-wiki-gigaword-100\")  # 100-dim embeddings\n",
    "embed_dim = 100\n",
    "\n",
    "# Build embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size, embed_dim))\n",
    "\n",
    "for word, idx in vocab.items():\n",
    "    if word in glove_model:\n",
    "        embedding_matrix[idx] = glove_model[word]\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embed_dim,))\n",
    "\n",
    "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80473966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequence(text, vocab, maxlen=200):\n",
    "    tokens = [t for t in nltk_tokenizer(text) if t.isalnum()]\n",
    "    seq = [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
    "    # Pad or truncate\n",
    "    if len(seq) < maxlen:\n",
    "        seq += [vocab[\"<pad>\"]] * (maxlen - len(seq))\n",
    "    else:\n",
    "        seq = seq[:maxlen]\n",
    "    return seq\n",
    "\n",
    "X_train_seq = torch.tensor([text_to_sequence(t, vocab) for t in X_train['text']], dtype=torch.long)\n",
    "X_test_seq = torch.tensor([text_to_sequence(t, vocab) for t in X_test['text']], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80d3f5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "bool_cols = [\"has_exclamation\", \"has_question\", \"is_shouting\"]\n",
    "sentiment_cols = [\"food_sentiment\", \"service_sentiment\", \"atmosphere_sentiment\", \"overall_sentiment\"]\n",
    "\n",
    "numeric_col = [\"grade_level\"]\n",
    "\n",
    "\n",
    "# Fit on training data\n",
    "onehot = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_cat = onehot.fit_transform(X_train[BOOLEAN_F + CATEGORICAL_F])\n",
    "X_test_cat = onehot.transform(X_test[BOOLEAN_F + CATEGORICAL_F])\n",
    "\n",
    "X_train_num = scaler.fit_transform(X_train[numeric_col])\n",
    "X_test_num = scaler.transform(X_test[numeric_col])\n",
    "\n",
    "# Combine\n",
    "X_train_meta = np.hstack([X_train_cat, X_train_num])\n",
    "X_test_meta = np.hstack([X_test_cat, X_test_num])\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_meta = torch.tensor(X_train_meta, dtype=torch.float32)\n",
    "X_test_meta = torch.tensor(X_test_meta, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46901213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ReviewsCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size, \n",
    "        embed_dim, \n",
    "        embedding_matrix,\n",
    "        meta_input_dim,\n",
    "        meta_hidden_dim=64,\n",
    "        num_filters=100, \n",
    "        kernel_sizes=[3,4,5], \n",
    "        text_hidden_dim=128,\n",
    "        combined_hidden_dim=128,\n",
    "        dropout=0.5,\n",
    "        num_class=5\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # -- Embedding --\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.embedding.weight = nn.Parameter(embedding_matrix)\n",
    "        self.embedding.weight.requires_grad = True\n",
    "\n",
    "        # -- CNN --\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=embed_dim, \n",
    "                out_channels=num_filters, \n",
    "                kernel_size=k,\n",
    "                padding=k//2\n",
    "            )\n",
    "            for k in kernel_sizes\n",
    "        ])\n",
    "        self.bns = nn.ModuleList([\n",
    "            nn.BatchNorm1d(num_filters) for _ in kernel_sizes\n",
    "        ])\n",
    "        self.text_dropout = nn.Dropout(dropout)\n",
    "        self.text_fc = nn.Linear(num_filters * len(kernel_sizes)*2, text_hidden_dim)\n",
    "\n",
    "        # -- Meta MLP --\n",
    "        self.meta_fc = nn.Sequential(\n",
    "            nn.Linear(meta_input_dim, meta_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(meta_hidden_dim, meta_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # -- Combined --\n",
    "        self.combined_fc = nn.Sequential(\n",
    "            nn.Linear(text_hidden_dim + meta_hidden_dim, combined_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(combined_hidden_dim, num_class)\n",
    "        )\n",
    "\n",
    "    def forward(self, text, meta):\n",
    "        # -- Text CNN -- \n",
    "        text = self.embedding(text)\n",
    "        text = text.permute(0, 2, 1)\n",
    "\n",
    "        conv_outputs = [\n",
    "            torch.relu(self.bns[i](self.convs[i](text)))\n",
    "            for i in range(len(self.convs))\n",
    "        ]\n",
    "        pooled = [\n",
    "                torch.cat([\n",
    "                    torch.max(out, dim=2)[0],\n",
    "                    torch.mean(out, dim=2)\n",
    "                ], dim=1)\n",
    "                for out in conv_outputs\n",
    "            ]\n",
    "        text_features = torch.cat(pooled, dim=1)\n",
    "        text_features = self.text_dropout(text_features)\n",
    "        text_hidden = self.text_fc(text_features)\n",
    "\n",
    "        # -- Meta branch -- \n",
    "        meta_features = self.meta_fc(meta)\n",
    "\n",
    "        # -- Combined --\n",
    "        combined = torch.cat([text_hidden, meta_features], dim=1)\n",
    "\n",
    "        # -- Final --\n",
    "        out = self.combined_fc(combined)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42ce9f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReviewsCNN(\n",
       "  (embedding): Embedding(10000, 100, padding_idx=0)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv1d(100, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): Conv1d(100, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  )\n",
       "  (bns): ModuleList(\n",
       "    (0-1): 2 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (text_dropout): Dropout(p=0.3, inplace=False)\n",
       "  (text_fc): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (meta_fc): Sequential(\n",
       "    (0): Linear(in_features=18, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (combined_fc): Sequential(\n",
       "    (0): Linear(in_features=384, out_features=300, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=300, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 100\n",
    "hidden_dim = 256\n",
    "meta_dim = X_train_meta.shape[1]\n",
    "output_dim = 5\n",
    "\n",
    "num_filters = 128\n",
    "kernel_sizes = [3,5]\n",
    "\n",
    "num_epochs = 50\n",
    "batch_size = 32\n",
    "accum_steps = 2\n",
    "patience = 5\n",
    "min_delta = 1e-4\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ReviewsCNN(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    meta_input_dim=meta_dim,\n",
    "    meta_hidden_dim=128,\n",
    "    num_filters=num_filters,\n",
    "    kernel_sizes=kernel_sizes,\n",
    "    text_hidden_dim=hidden_dim,\n",
    "    combined_hidden_dim=300,\n",
    "    dropout=0.3,\n",
    "    num_class=output_dim\n",
    ").to(device)\n",
    "\n",
    "# Load weights onto CPU\n",
    "model.load_state_dict(torch.load(\"best_cnn_model.pth\", map_location=torch.device('cpu')))\n",
    "model.eval()  \n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "609fd63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, text_tensors, meta_features, labels):\n",
    "        self.text_tensors = text_tensors\n",
    "        self.meta_features = meta_features\n",
    "\n",
    "        if isinstance(labels, (pd.Series, pd.DataFrame)):\n",
    "            self.labels = torch.tensor(labels.values - 1, dtype=torch.long)\n",
    "        else:\n",
    "            self.labels = labels.long()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text_tensors[idx]\n",
    "        meta = self.meta_features[idx]\n",
    "        label = self.labels[idx]\n",
    "        return text, meta, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5dad6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Shift labels 1-5 â†’ 0-4\n",
    "y_train_tensor = torch.tensor(y_train.values - 1, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test.values - 1, dtype=torch.long)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ReviewDataset(X_train_seq, X_train_meta, y_train_tensor)\n",
    "test_dataset = ReviewDataset(X_test_seq, X_test_meta, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45b08d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for text_batch, meta_batch, _ in test_loader:\n",
    "        text_batch = text_batch.to(device)\n",
    "        meta_batch = meta_batch.to(device)\n",
    "        outputs = model(text_batch, meta_batch)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "# Keep both on 0-4 scale\n",
    "y_pred = np.array(y_pred)\n",
    "y_true = y_test_tensor.numpy()  # already 0-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf1d79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Misclassification Analysis Complete.\n",
      "Results saved to 'misclassification_analysis_cnn.csv' for your next step.\n",
      "\n",
      "--- Model Performance Summary ---\n",
      "Accuracy: 0.6631127593000348\n",
      "Sample of Misclassified Reviews (True vs. Predicted):\n",
      "              business_id  True_Star_Rating  Predicted_Star_Rating\n",
      "0  V7IHpr1xzFIf_jp876HoAw                 4                      3\n",
      "1  V7IHpr1xzFIf_jp876HoAw                 5                      4\n",
      "2  V7IHpr1xzFIf_jp876HoAw                 5                      3\n",
      "4  s9G06FPW74Prlp8s1h5nEA                 5                      4\n",
      "5  s9G06FPW74Prlp8s1h5nEA                 4                      3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# Create the final DataFrame for misclassification analysis\n",
    "misclassification_df = pd.DataFrame({\n",
    "    'business_id': business_ids_test.values,\n",
    "    'Review_Text': X_test[TEXT_COL].values,\n",
    "    'True_Star_Rating': y_test.values,\n",
    "    'Predicted_Star_Rating': y_pred\n",
    "})\n",
    "\n",
    "misclassification_df['Is_Misclassified'] = (misclassification_df['True_Star_Rating'] != misclassification_df['Predicted_Star_Rating'])\n",
    "\n",
    "# Save the DataFrame to a file\n",
    "OUTPUT_FILE = 'misclassification_analysis_cnn.csv'\n",
    "misclassification_df.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(\"\\nMisclassification Analysis Complete.\")\n",
    "print(f\"Results saved to '{OUTPUT_FILE}' for your next step.\")\n",
    "print(\"\\n--- Model Performance Summary ---\")\n",
    "print(\"Accuracy:\", classification_report(y_true, y_pred, output_dict=True)['accuracy'])\n",
    "print(\"Sample of Misclassified Reviews (True vs. Predicted):\")\n",
    "print(misclassification_df[misclassification_df['Is_Misclassified']][['business_id', 'True_Star_Rating', 'Predicted_Star_Rating']].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
