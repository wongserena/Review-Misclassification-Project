{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe6114f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 11:25:45.592958: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Deep Learning specific imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPool1D, Dense, concatenate\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# --- 1. Define Constants and Features ---\n",
    "FILE_NAME = '../4-prep_model_data/modelling_data.csv'\n",
    "TARGET_COL = 'stars_x'\n",
    "GROUP_COL = 'business_id'\n",
    "TEXT_COL = 'text'\n",
    "\n",
    "# Feature lists\n",
    "BOOLEAN_F = ['has_exclamation', 'has_question', 'is_shouting']\n",
    "CATEGORICAL_F = ['food_sentiment', 'service_sentiment', 'atmosphere_sentiment', 'overall_sentiment']\n",
    "NUMERICAL_F = ['grade_level']\n",
    "ALL_INPUT_FEATURES = [TEXT_COL] + BOOLEAN_F + CATEGORICAL_F + NUMERICAL_F\n",
    "\n",
    "# Deep Learning Hyperparameters\n",
    "MAX_WORDS = 10000        # Max vocabulary size for the tokenizer\n",
    "MAX_SEQ_LENGTH = 150     # Max length of a review (sentences longer than this are truncated)\n",
    "EMBEDDING_DIM = 100      # Size of the word embedding vector\n",
    "NUM_CLASSES = 5          # Number of star ratings (1 to 5)\n",
    "FILTERS = 128            # Number of filters (feature detectors) for the CNN\n",
    "KERNEL_SIZES = [3, 4, 5] # Size of the n-grams the CNN will look for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4478c18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from '4-prep_model_data/modelling_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Load and Prepare Data ---\n",
    "try:\n",
    "    df = pd.read_csv(FILE_NAME)\n",
    "    print(f\"Data loaded successfully from '{FILE_NAME}'.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File '{FILE_NAME}' not found. Please ensure the file is uploaded.\")\n",
    "    exit()\n",
    "\n",
    "# Data Cleaning and Preparation\n",
    "df.dropna(subset=[TARGET_COL, GROUP_COL, TEXT_COL], inplace=True)\n",
    "df[CATEGORICAL_F] = df[CATEGORICAL_F].fillna('missing_category')\n",
    "df[BOOLEAN_F] = df[BOOLEAN_F].fillna(False)\n",
    "df[NUMERICAL_F] = df[NUMERICAL_F].fillna(df[NUMERICAL_F].mean()) \n",
    "\n",
    "for col in BOOLEAN_F:\n",
    "    df[col] = df[col].astype(str)\n",
    "\n",
    "# Map stars_x (1 to 5) to classes (0 to 4) for categorical cross-entropy loss\n",
    "# Keras requires class indices starting from 0\n",
    "df['class_label'] = df[TARGET_COL] - 1 \n",
    "\n",
    "y = df['class_label'] \n",
    "X = df[ALL_INPUT_FEATURES]\n",
    "groups = df[GROUP_COL] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26694ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Size: 37828 | Testing Set Size: 8629\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Stratified Group Split (80/20) ---\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "try:\n",
    "    train_index, test_index = next(sgkf.split(X, y, groups))\n",
    "except ValueError as e:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    print(\"\\nWARNING: StratifiedGroupKFold failed. Falling back to standard stratified split.\")\n",
    "    # Use the class labels for stratification\n",
    "    train_index, test_index = train_test_split(df.index, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "business_ids_test = groups.loc[test_index] \n",
    "\n",
    "print(f\"\\nTraining Set Size: {len(X_train)} | Testing Set Size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39dd38f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing Deep Learning Text Preprocessing...\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Deep Learning Text Preprocessing (Tokenizer and Padding) ---\n",
    "print(\"\\nPerforming Deep Learning Text Preprocessing...\")\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<unk>\")\n",
    "tokenizer.fit_on_texts(X_train[TEXT_COL])\n",
    "\n",
    "# Convert text to sequences (integers)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train[TEXT_COL])\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test[TEXT_COL])\n",
    "\n",
    "# Pad sequences (make all reviews the same length)\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=MAX_SEQ_LENGTH, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=MAX_SEQ_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "# Convert target labels to one-hot encoding (required for multi-class classification in Keras)\n",
    "Y_train_categorical = to_categorical(y_train, num_classes=NUM_CLASSES)\n",
    "Y_test_categorical = to_categorical(y_test, num_classes=NUM_CLASSES)\n",
    "VOCAB_SIZE = len(tokenizer.word_index) + 1 # Actual size of the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "433c0621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Preprocess Meta Features (One-Hot and Scaling) ---\n",
    "# We use the same preprocessing setup as before for consistency\n",
    "preprocessor_meta = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat_pipe', OneHotEncoder(handle_unknown='ignore'), CATEGORICAL_F + BOOLEAN_F),\n",
    "        ('num_pipe', StandardScaler(), NUMERICAL_F)\n",
    "    ],\n",
    "    remainder='drop' \n",
    ")\n",
    "\n",
    "# Fit and transform the meta features\n",
    "X_train_meta = preprocessor_meta.fit_transform(X_train[BOOLEAN_F + CATEGORICAL_F + NUMERICAL_F])\n",
    "X_test_meta = preprocessor_meta.transform(X_test[BOOLEAN_F + CATEGORICAL_F + NUMERICAL_F])\n",
    "META_FEATURE_DIM = X_train_meta.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39a09534",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptimizers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[32m      5\u001b[39m text_input = Input(shape=(MAX_SEQ_LENGTH,), name=\u001b[33m'\u001b[39m\u001b[33mtext_input\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m x = Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM,\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m               input_length=MAX_SEQ_LENGTH, weights=[\u001b[43membedding_matrix\u001b[49m], trainable=\u001b[38;5;28;01mFalse\u001b[39;00m)(text_input)\n\u001b[32m      8\u001b[39m x = SpatialDropout1D(\u001b[32m0.2\u001b[39m)(x)\n\u001b[32m     10\u001b[39m conv_blocks = []\n",
      "\u001b[31mNameError\u001b[39m: name 'embedding_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPool1D, Dense, concatenate, Dropout, SpatialDropout1D, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "text_input = Input(shape=(MAX_SEQ_LENGTH,), name='text_input')\n",
    "x = Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM,\n",
    "              input_length=MAX_SEQ_LENGTH, weights=[embedding_matrix], trainable=False)(text_input)\n",
    "x = SpatialDropout1D(0.2)(x)\n",
    "\n",
    "conv_blocks = []\n",
    "for k in KERNEL_SIZES:\n",
    "    conv = Conv1D(filters=FILTERS, kernel_size=k, activation='relu')(x)\n",
    "    conv = Conv1D(filters=FILTERS, kernel_size=k, activation='relu')(conv)\n",
    "    pool = GlobalMaxPool1D()(conv)\n",
    "    conv_blocks.append(pool)\n",
    "\n",
    "cnn_output = concatenate(conv_blocks)\n",
    "cnn_output = Dense(128, activation='relu')(cnn_output)\n",
    "cnn_output = BatchNormalization()(cnn_output)\n",
    "cnn_output = Dropout(0.5)(cnn_output)\n",
    "\n",
    "meta_input = Input(shape=(META_FEATURE_DIM,), name='meta_input')\n",
    "meta_output = Dense(16, activation='relu')(meta_input)\n",
    "\n",
    "combined = concatenate([cnn_output, meta_output])\n",
    "combined = Dense(64, activation='relu')(combined)\n",
    "combined = Dropout(0.4)(combined)\n",
    "final_output = Dense(NUM_CLASSES, activation='softmax')(combined)\n",
    "\n",
    "model = Model(inputs=[text_input, meta_input], outputs=final_output)\n",
    "optimizer = Adam(learning_rate=1e-4)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e69e7529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting TextCNN Training...\n",
      "Epoch 1/10\n",
      "\u001b[1m1064/1064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 65ms/step - accuracy: 0.4311 - loss: 1.3820 - val_accuracy: 0.4832 - val_loss: 1.1744\n",
      "Epoch 2/10\n",
      "\u001b[1m1064/1064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 64ms/step - accuracy: 0.5250 - loss: 1.0919 - val_accuracy: 0.6196 - val_loss: 0.9042\n",
      "Epoch 3/10\n",
      "\u001b[1m1064/1064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 64ms/step - accuracy: 0.5954 - loss: 0.9298 - val_accuracy: 0.6460 - val_loss: 0.8241\n",
      "Epoch 4/10\n",
      "\u001b[1m1064/1064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 63ms/step - accuracy: 0.6371 - loss: 0.8434 - val_accuracy: 0.6667 - val_loss: 0.7795\n",
      "Epoch 5/10\n",
      "\u001b[1m1064/1064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 65ms/step - accuracy: 0.6669 - loss: 0.7759 - val_accuracy: 0.6669 - val_loss: 0.7621\n",
      "Epoch 6/10\n",
      "\u001b[1m1064/1064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 61ms/step - accuracy: 0.6982 - loss: 0.7130 - val_accuracy: 0.6696 - val_loss: 0.7581\n",
      "Epoch 7/10\n",
      "\u001b[1m1064/1064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 63ms/step - accuracy: 0.7311 - loss: 0.6503 - val_accuracy: 0.6646 - val_loss: 0.7702\n",
      "Epoch 8/10\n",
      "\u001b[1m1064/1064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 65ms/step - accuracy: 0.7672 - loss: 0.5807 - val_accuracy: 0.6646 - val_loss: 0.7850\n",
      "Epoch 9/10\n",
      "\u001b[1m1064/1064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 65ms/step - accuracy: 0.8098 - loss: 0.5031 - val_accuracy: 0.6553 - val_loss: 0.8249\n",
      "Epoch 10/10\n",
      "\u001b[1m1064/1064\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 65ms/step - accuracy: 0.8494 - loss: 0.4199 - val_accuracy: 0.6524 - val_loss: 0.8724\n",
      "Training complete!\n",
      "\n",
      "Test Accuracy (TextCNN): 0.6373\n",
      "\u001b[1m270/270\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step\n",
      "\n",
      "==================================================\n",
      "CLASSIFICATION REPORT (TextCNN: Text + Meta)\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.72      0.74      1137\n",
      "           2       0.43      0.43      0.43       755\n",
      "           3       0.47      0.42      0.44      1024\n",
      "           4       0.50      0.57      0.53      2089\n",
      "           5       0.79      0.75      0.77      3624\n",
      "\n",
      "    accuracy                           0.64      8629\n",
      "   macro avg       0.59      0.58      0.58      8629\n",
      "weighted avg       0.64      0.64      0.64      8629\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Model Training and Evaluation ---\n",
    "print(\"\\nStarting TextCNN Training...\")\n",
    "# Use a validation split to monitor for overfitting\n",
    "history = model.fit(\n",
    "    [X_train_padded, X_train_meta], \n",
    "    Y_train_categorical,\n",
    "    epochs=10, \n",
    "    batch_size=32, \n",
    "    validation_split=0.1, # Monitor performance on 10% of the training data\n",
    "    verbose=1\n",
    ")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "loss, accuracy = model.evaluate([X_test_padded, X_test_meta], Y_test_categorical, verbose=0)\n",
    "print(f\"\\nTest Accuracy (TextCNN): {accuracy:.4f}\")\n",
    "\n",
    "# Generate predictions for the classification report and analysis\n",
    "y_pred_proba = model.predict([X_test_padded, X_test_meta])\n",
    "y_pred_labels = np.argmax(y_pred_proba, axis=1) # Convert one-hot back to single label (0 to 4)\n",
    "y_true_labels = np.argmax(Y_test_categorical, axis=1) # True labels (0 to 4)\n",
    "\n",
    "# Revert labels to original star rating (1 to 5) for the report\n",
    "y_pred_stars = y_pred_labels + 1\n",
    "y_true_stars = y_true_labels + 1\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CLASSIFICATION REPORT (TextCNN: Text + Meta)\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(y_true_stars, y_pred_stars, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41b0e2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Misclassification Analysis Complete for TextCNN.\n",
      "Results saved to 'misclassification_analysis_textcnn.csv'.\n",
      "\n",
      "Sample of Misclassified Reviews (True vs. Predicted):\n",
      "               business_id  True_Star_Rating  Predicted_Star_Rating\n",
      "1   V7IHpr1xzFIf_jp876HoAw                 5                      4\n",
      "3   V7IHpr1xzFIf_jp876HoAw                 3                      5\n",
      "9   s9G06FPW74Prlp8s1h5nEA                 3                      2\n",
      "15  41RbEZa99W2d_kTnYTp_mw                 5                      4\n",
      "17  41RbEZa99W2d_kTnYTp_mw                 4                      5\n"
     ]
    }
   ],
   "source": [
    "# --- 8. Misclassification Analysis Prep ---\n",
    "misclassification_df_cnn = pd.DataFrame({\n",
    "    'business_id': business_ids_test.values,\n",
    "    'Review_Text': X_test[TEXT_COL].values,\n",
    "    'True_Star_Rating': y_true_stars,\n",
    "    'Predicted_Star_Rating': y_pred_stars\n",
    "})\n",
    "\n",
    "misclassification_df_cnn['Is_Misclassified'] = (misclassification_df_cnn['True_Star_Rating'] != misclassification_df_cnn['Predicted_Star_Rating'])\n",
    "\n",
    "OUTPUT_FILE = 'misclassification_analysis_textcnn.csv'\n",
    "misclassification_df_cnn.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(\"\\nMisclassification Analysis Complete for TextCNN.\")\n",
    "print(f\"Results saved to '{OUTPUT_FILE}'.\")\n",
    "print(\"\\nSample of Misclassified Reviews (True vs. Predicted):\")\n",
    "print(misclassification_df_cnn[misclassification_df_cnn['Is_Misclassified']][['business_id', 'True_Star_Rating', 'Predicted_Star_Rating']].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
