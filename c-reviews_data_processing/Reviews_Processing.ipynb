{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97a6ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047591fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"restaurant_reviews_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2bc409",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0c2df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[\"state\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2307dc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[\"stars_x\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2593b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1670c231",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[\"has_exclamation\"] = reviews[\"text\"].fillna(\"\").str.contains(\"!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a1d0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.drop([\"useful\", \"funny\", \"cool\", \"address\", \"postal_code\", \"latitude\", \"longitude\", \"is_open\", \"stars_y\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ad5495",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[\"has_question\"] = reviews[\"text\"].fillna(\"\").str.contains(\"?\", regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2b57fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if review has uppercase words aka shouting\n",
    "reviews[\"uppercase_ratio\"] = reviews[\"text\"].fillna(\"\").apply(\n",
    "    lambda x: sum(c.isupper() for c in x) / max(1, len(x))\n",
    ")\n",
    "reviews[\"is_shouting\"] = reviews[\"uppercase_ratio\"] > 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24f6c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[reviews[\"is_shouting\"]][\"text\"].iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24cbb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b900b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[\"is_shouting\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020df232",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[\"length\"] = reviews[\"text\"].fillna(\"\").apply(len)\n",
    "reviews[\"length\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab629fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, DetectorFactory\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == \"en\"\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "reviews = reviews[reviews[\"text\"].fillna(\"\").apply(is_english)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c84064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "texts = reviews[\"text\"].fillna(\"\").tolist()\n",
    "\n",
    "# -----------------------------\n",
    "# Setup ABSA pipeline\n",
    "# -----------------------------\n",
    "absa_model_name = \"yangheng/distilbert-base-uncased-absa\"\n",
    "\n",
    "absa_tokenizer = AutoTokenizer.from_pretrained(absa_model_name, use_fast=False)\n",
    "absa_model = AutoModelForSequenceClassification.from_pretrained(absa_model_name)\n",
    "\n",
    "absa_pipeline = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=absa_model,\n",
    "    tokenizer=absa_tokenizer,\n",
    "    device=-1,  # CPU,\n",
    "    truncation=True,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "# Overall sentiment pipeline (smaller model, faster)\n",
    "overall_pipeline = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    device=-1,  # CPU,\n",
    "    truncation=True,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Batch processing function\n",
    "# -----------------------------\n",
    "def batch_process(texts, pipe, batch_size=32):\n",
    "    results = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing batches\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        batch_results = pipe(batch)\n",
    "        results.extend(batch_results)\n",
    "    return results\n",
    "\n",
    "# -----------------------------\n",
    "# ABSA for each aspect\n",
    "# -----------------------------\n",
    "aspects = [\"food\", \"service\", \"atmosphere\"]\n",
    "\n",
    "# Sort by length for speed\n",
    "sorted_indices = sorted(range(len(texts)), key=lambda i: len(texts[i]))\n",
    "texts_sorted = [texts[i] for i in sorted_indices]\n",
    "\n",
    "for aspect in aspects:\n",
    "    print(f\"Processing aspect: {aspect}\")\n",
    "    absa_inputs = [f\"{t} [ASP] {aspect}\" for t in texts_sorted]\n",
    "    absa_results = batch_process(absa_inputs, absa_pipeline, batch_size=16)\n",
    "    \n",
    "    # Extract labels\n",
    "    labels = [r[\"label\"] for r in absa_results]\n",
    "    \n",
    "    # Put back in original order\n",
    "    col = [None]*len(texts)\n",
    "    for idx, label in zip(sorted_indices, labels):\n",
    "        col[idx] = label\n",
    "    reviews[f\"{aspect}_sentiment\"] = col\n",
    "\n",
    "# -----------------------------\n",
    "# Overall sentiment\n",
    "# -----------------------------\n",
    "overall_results = batch_process(texts_sorted, overall_pipeline, batch_size=64)\n",
    "labels = [r[\"label\"] for r in overall_results]\n",
    "\n",
    "overall_col = [None]*len(texts)\n",
    "for idx, label in zip(sorted_indices, labels):\n",
    "    overall_col[idx] = label\n",
    "reviews[\"overall_sentiment\"] = overall_col\n",
    "\n",
    "print(\"Done! ABSA and overall sentiment stored in DataFrame.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
