{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4ac2824",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StratifiedGroupKFold\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# --- 1. Define Constants and Features ---\n",
    "FILE_NAME = '../4-prep_model_data/modelling_data.csv'\n",
    "TARGET_COL = 'stars_x'\n",
    "GROUP_COL = 'business_id'\n",
    "TEXT_COL = 'text'\n",
    "\n",
    "# The additional features you requested to include\n",
    "BOOLEAN_F = ['has_exclamation', 'has_question', 'is_shouting']\n",
    "CATEGORICAL_F = ['food_sentiment', 'service_sentiment', 'atmosphere_sentiment', 'overall_sentiment']\n",
    "NUMERICAL_F = ['grade_level']\n",
    "\n",
    "# All features that will be used as input (X)\n",
    "ALL_INPUT_FEATURES = [TEXT_COL] + BOOLEAN_F + CATEGORICAL_F + NUMERICAL_F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcf4e8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from 'modelling_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Load and Prepare Data ---\n",
    "try:\n",
    "    df = pd.read_csv(FILE_NAME)\n",
    "    print(f\"Data loaded successfully from '{FILE_NAME}'.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File '{FILE_NAME}' not found. Please ensure the file is uploaded.\")\n",
    "    # Exit the script if the file cannot be loaded\n",
    "    exit()\n",
    "\n",
    "# Data Cleaning and Preparation for Robust Modeling\n",
    "df.dropna(subset=[TARGET_COL, GROUP_COL, TEXT_COL], inplace=True)\n",
    "# Fill NaNs for categorical/boolean/numerical columns to prevent data loss in the remaining rows\n",
    "df[CATEGORICAL_F] = df[CATEGORICAL_F].fillna('missing_category')\n",
    "df[BOOLEAN_F] = df[BOOLEAN_F].fillna(False)\n",
    "df[NUMERICAL_F] = df[NUMERICAL_F].fillna(df[NUMERICAL_F].mean()) # Fill numerical NaNs with the mean\n",
    "\n",
    "# Convert boolean columns to string/object type for proper One-Hot Encoding\n",
    "for col in BOOLEAN_F:\n",
    "    df[col] = df[col].astype(str)\n",
    "\n",
    "# Define X, y, and groups after cleaning\n",
    "y = df[TARGET_COL] \n",
    "X = df[ALL_INPUT_FEATURES]\n",
    "groups = df[GROUP_COL] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea6100cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Size: 37828 | Testing Set Size: 8629\n",
      "Test set unique businesses: 134\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 3. Stratified Group Split (80/20) ---\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "try:\n",
    "    # Get the indices for the 80/20 split, respecting star rating stratification and business grouping\n",
    "    train_index, test_index = next(sgkf.split(X, y, groups))\n",
    "except ValueError as e:\n",
    "    # Fallback if a group or class is too small to stratify\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    print(\"\\nWARNING: StratifiedGroupKFold failed. Falling back to standard stratified split.\")\n",
    "    train_index, test_index = train_test_split(df.index, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Apply the indices to create the training and testing sets\n",
    "X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "business_ids_test = groups.loc[test_index] \n",
    "\n",
    "print(f\"\\nTraining Set Size: {len(X_train)} | Testing Set Size: {len(X_test)}\")\n",
    "print(f\"Test set unique businesses: {business_ids_test.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c1475d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Build the ColumnTransformer (The Feature Combiner) ---\n",
    "# This defines how each type of feature is preprocessed\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # 1. Text Feature (TF-IDF)\n",
    "        ('text_pipe', TfidfVectorizer(\n",
    "            stop_words='english',\n",
    "            ngram_range=(1, 2),        # Use unigrams and bigrams\n",
    "            max_features=10000         # Limit vocabulary size\n",
    "        ), TEXT_COL),\n",
    "        \n",
    "        # 2. Categorical and Boolean Features (One-Hot Encoding)\n",
    "        ('cat_pipe', OneHotEncoder(handle_unknown='ignore'), CATEGORICAL_F + BOOLEAN_F),\n",
    "        \n",
    "        # 3. Numerical Features (Scaling)\n",
    "        ('num_pipe', StandardScaler(), NUMERICAL_F)\n",
    "    ],\n",
    "    remainder='drop' \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "863fc22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting model training with ALL combined features (Text, Sentiment, Meta Data)...\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Build and Train the Full SVM Pipeline --- \n",
    "# The Pipeline chains the preprocessing step and the SVM classifier\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('svm', SVC(kernel='linear', C=1.0, random_state=42)) \n",
    "])\n",
    "\n",
    "print(\"\\nStarting model training with ALL combined features (Text, Sentiment, Meta Data)...\")\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e826a3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "CLASSIFICATION REPORT (Combined Features: Text + Meta)\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.71      0.78      0.74      1137\n",
      "           2       0.45      0.41      0.43       755\n",
      "           3       0.50      0.35      0.41      1024\n",
      "           4       0.51      0.48      0.49      2089\n",
      "           5       0.75      0.83      0.79      3624\n",
      "\n",
      "    accuracy                           0.64      8629\n",
      "   macro avg       0.59      0.57      0.57      8629\n",
      "weighted avg       0.63      0.64      0.63      8629\n",
      "\n",
      "\n",
      "Misclassification Analysis Complete.\n",
      "Results saved to 'misclassification_analysis_combined_features.csv' for your next step.\n",
      "\n",
      "--- Model Performance Summary ---\n",
      "Accuracy: 0.6448024104763008\n",
      "Sample of Misclassified Reviews (True vs. Predicted):\n",
      "              business_id  True_Star_Rating  Predicted_Star_Rating\n",
      "0  V7IHpr1xzFIf_jp876HoAw                 4                      3\n",
      "1  V7IHpr1xzFIf_jp876HoAw                 5                      4\n",
      "2  V7IHpr1xzFIf_jp876HoAw                 5                      4\n",
      "3  V7IHpr1xzFIf_jp876HoAw                 3                      5\n",
      "6  s9G06FPW74Prlp8s1h5nEA                 4                      5\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Evaluate and Prepare for Misclassification Analysis ---\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CLASSIFICATION REPORT (Combined Features: Text + Meta)\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "# Create the final DataFrame for misclassification analysis\n",
    "misclassification_df = pd.DataFrame({\n",
    "    'business_id': business_ids_test.values,\n",
    "    'Review_Text': X_test[TEXT_COL].values,\n",
    "    'True_Star_Rating': y_test.values,\n",
    "    'Predicted_Star_Rating': y_pred\n",
    "})\n",
    "\n",
    "misclassification_df['Is_Misclassified'] = (misclassification_df['True_Star_Rating'] != misclassification_df['Predicted_Star_Rating'])\n",
    "\n",
    "# Save the DataFrame to a file for your detailed analysis\n",
    "OUTPUT_FILE = 'misclassification_analysis_combined_features.csv'\n",
    "misclassification_df.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(\"\\nMisclassification Analysis Complete.\")\n",
    "print(f\"Results saved to '{OUTPUT_FILE}' for your next step.\")\n",
    "print(\"\\n--- Model Performance Summary ---\")\n",
    "print(\"Accuracy:\", classification_report(y_test, y_pred, output_dict=True)['accuracy'])\n",
    "print(\"Sample of Misclassified Reviews (True vs. Predicted):\")\n",
    "print(misclassification_df[misclassification_df['Is_Misclassified']][['business_id', 'True_Star_Rating', 'Predicted_Star_Rating']].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
